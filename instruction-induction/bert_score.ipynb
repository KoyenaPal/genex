{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba5a50ea-475e-4f30-88e6-3d60699f388f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Calculating scores for: nvidia_Nemotron-Research-Reasoning-Qwen-1.5B with with_sampling_without_answer thoughts\n",
      "Looking in folder: predictions_nvidia_Nemotron-Research-Reasoning-Qwen-1.5B_with_sampling_without_answer\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Total tasks processed: 20/20\n",
      "TOTAL AVERAGE SCORE: 0.5593\n",
      "\n",
      "============================================================\n",
      "Calculating scores for: open-thoughts_OpenThinker-7B with with_sampling_without_answer thoughts\n",
      "Looking in folder: predictions_open-thoughts_OpenThinker-7B_with_sampling_without_answer\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Total tasks processed: 20/20\n",
      "TOTAL AVERAGE SCORE: 0.5648\n",
      "\n",
      "============================================================\n",
      "Calculating scores for: openai_gpt-oss-20b with with_sampling_without_answer thoughts\n",
      "Looking in folder: predictions_openai_gpt-oss-20b_with_sampling_without_answer\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Total tasks processed: 20/20\n",
      "TOTAL AVERAGE SCORE: 0.5986\n",
      "\n",
      "============================================================\n",
      "Calculating scores for: Qwen_QwQ-32B with with_sampling_without_answer thoughts\n",
      "Looking in folder: predictions_Qwen_QwQ-32B_with_sampling_without_answer\n",
      "--------------------------------------------------\n",
      "cause_and_effect                   : File not found\n",
      "larger_animal                      : File not found\n",
      "num_to_verbal                      : File not found\n",
      "orthography_starts_with            : File not found\n",
      "rhymes                             : File not found\n",
      "synonyms                           : File not found\n",
      "taxonomy_animal                    : File not found\n",
      "translation_en-fr                  : File not found\n",
      "reverse_from_middle                : File not found\n",
      "smallest_item_length               : File not found\n",
      "smallest_even_no_sqrt              : File not found\n",
      "most_vowel_return_consonant        : File not found\n",
      "detect_rhyme_and_rewrite           : File not found\n",
      "rank_by_protein                    : File not found\n",
      "multi_lang_to_english              : File not found\n",
      "square_of_zodiac_animal            : File not found\n",
      "alternate_synonym_antonym          : File not found\n",
      "most_consonant_return_vowel        : File not found\n",
      "least_unique_word_count            : File not found\n",
      "first_word_alphabetically_return_reverse: File not found\n",
      "--------------------------------------------------\n",
      "No valid scores found!\n",
      "\n",
      "============================================================\n",
      "Calculating scores for: BytedTsinghua-SIA_DAPO-Qwen-32B with with_sampling_without_answer thoughts\n",
      "Looking in folder: predictions_BytedTsinghua-SIA_DAPO-Qwen-32B_with_sampling_without_answer\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Total tasks processed: 20/20\n",
      "TOTAL AVERAGE SCORE: 0.6046\n",
      "\n",
      "============================================================\n",
      "SUMMARY COMPARISON\n",
      "============================================================\n",
      "nvidia_Nemotron-Research-Reasoning-Qwen-1.5B(with_sampling_without_answer): 0.5593 (20/20 tasks)\n",
      "open-thoughts_OpenThinker-7B(with_sampling_without_answer): 0.5648 (20/20 tasks)\n",
      "openai_gpt-oss-20b(with_sampling_without_answer): 0.5986 (20/20 tasks)\n",
      "Qwen_QwQ-32B                       : No valid scores\n",
      "BytedTsinghua-SIA_DAPO-Qwen-32B(with_sampling_without_answer): 0.6046 (20/20 tasks)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def calculate_average_induction_scores(model_name, thought_type=\"\", source_model=None):\n",
    "    \"\"\"\n",
    "    Calculate the total average score across all induction tasks for a given model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model (used in folder name)\n",
    "        thought_type (str): Type of thought process (\"\", \"empty\", \"transfer\", etc.)\n",
    "        source_model (str): Source model name (required when thought_type=\"transfer\")\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains individual task scores and overall average\n",
    "    \"\"\"\n",
    "    \n",
    "    INDUCTION_TASKS = [\n",
    "        'cause_and_effect', 'larger_animal', 'num_to_verbal','orthography_starts_with',\n",
    "        'rhymes', 'synonyms', 'taxonomy_animal', 'translation_en-fr',\n",
    "        'reverse_from_middle', 'smallest_item_length', 'smallest_even_no_sqrt', 'most_vowel_return_consonant',\n",
    "        'detect_rhyme_and_rewrite', 'rank_by_protein','multi_lang_to_english','square_of_zodiac_animal',\n",
    "        'alternate_synonym_antonym', 'most_consonant_return_vowel', 'least_unique_word_count', 'first_word_alphabetically_return_reverse'\n",
    "    ]\n",
    "    \n",
    "    # Determine folder name based on thought_type\n",
    "    if thought_type == \"transfer\":\n",
    "        if source_model is None:\n",
    "            raise ValueError(\"source_model must be provided when thought_type='transfer'\")\n",
    "        predictions_folder = Path(f\"predictions_{source_model}_thoughts_to_{model_name}\")\n",
    "        folder_description = f\"transfer from {source_model} to {model_name}\"\n",
    "    elif thought_type == \"transfer_without_answer\":\n",
    "        if source_model is None:\n",
    "            raise ValueError(\"source_model must be provided when thought_type='transfer'\")\n",
    "        if \"dapo\" in source_model.lower():\n",
    "            predictions_folder = Path(f\"predictions_without_answer_dapo_thoughts_to_{model_name}\")\n",
    "        elif \"oss\" in source_model.lower():\n",
    "            predictions_folder = Path(f\"predictions_without_answer_oss_thoughts_to_{model_name}\")\n",
    "        elif \"qwq\" in source_model.lower():\n",
    "            predictions_folder = Path(f\"predictions_without_answer_qwq_thoughts_to_{model_name}\")\n",
    "        elif \"open-thoughts\" in source_model.lower():\n",
    "            predictions_folder = Path(f\"predictions_without_answer_opent_thoughts_to_{model_name}\")\n",
    "        elif \"nemotron\" in source_model.lower():\n",
    "            predictions_folder = Path(f\"predictions_without_answer_nrr_thoughts_to_{model_name}\")\n",
    "        else:\n",
    "            predictions_folder = Path(f\"predictions_{source_model}_thoughts_to_{model_name}\")\n",
    "        folder_description = f\"transfer from {source_model} to {model_name}\"\n",
    "    else:\n",
    "        thought_suffix = f\"_{thought_type}\" if thought_type else \"\"\n",
    "        predictions_folder = Path(f\"predictions_{model_name}{thought_suffix}\")\n",
    "        folder_description = f\"{model_name} with {thought_type if thought_type else 'default'} thoughts\"\n",
    "            \n",
    "    \n",
    "    task_scores = {}\n",
    "    missing_files = []\n",
    "    \n",
    "    print(f\"Calculating scores for: {folder_description}\")\n",
    "    print(f\"Looking in folder: {predictions_folder}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not predictions_folder.exists():\n",
    "        print(f\"Error: Folder '{predictions_folder}' does not exist!\")\n",
    "        return None\n",
    "    \n",
    "    # Process each task\n",
    "    for task in INDUCTION_TASKS:\n",
    "        file_path = predictions_folder / f\"{task}_with_scores.json\"\n",
    "        \n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Extract weighted task score\n",
    "                if 'weighted_task_score' in data:\n",
    "                    score = data['weighted_task_score']\n",
    "                    task_scores[task] = score\n",
    "                    # print(f\"{task:<35}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"{task:<35}: Missing 'weighted_task_score' key\")\n",
    "                    missing_files.append(f\"{task} (missing key)\")\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"{task:<35}: Error reading JSON file\")\n",
    "                missing_files.append(f\"{task} (JSON error)\")\n",
    "            except Exception as e:\n",
    "                print(f\"{task:<35}: Error - {str(e)}\")\n",
    "                missing_files.append(f\"{task} (error)\")\n",
    "        else:\n",
    "            print(f\"{task:<35}: File not found\")\n",
    "            missing_files.append(task)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate average\n",
    "    if task_scores:\n",
    "        total_average = sum(task_scores.values()) / len(task_scores)\n",
    "        print(f\"Total tasks processed: {len(task_scores)}/{len(INDUCTION_TASKS)}\")\n",
    "        print(f\"TOTAL AVERAGE SCORE: {total_average:.4f}\")\n",
    "        \n",
    "        if missing_files:\n",
    "            print(f\"\\nMissing/Error files ({len(missing_files)}):\")\n",
    "            for missing in missing_files:\n",
    "                print(f\"  - {missing}\")\n",
    "    else:\n",
    "        print(\"No valid scores found!\")\n",
    "        total_average = None\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'source_model': source_model,\n",
    "        'thought_type': thought_type,\n",
    "        'task_scores': task_scores,\n",
    "        'total_average': total_average,\n",
    "        'tasks_processed': len(task_scores),\n",
    "        'total_tasks': len(INDUCTION_TASKS),\n",
    "        'missing_files': missing_files,\n",
    "        'folder_path': str(predictions_folder)\n",
    "    }\n",
    "\n",
    "def compare_multiple_models(model_names, thought_type=\"\", source_model=None):\n",
    "    \"\"\"\n",
    "    Compare average scores across multiple models.\n",
    "    \n",
    "    Args:\n",
    "        model_names (list): List of model names to compare\n",
    "        thought_type (str): Type of thought process\n",
    "        source_model (str): Source model name (required when thought_type=\"transfer\")\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        result = calculate_average_induction_scores(model_name, thought_type, source_model)\n",
    "        if result:\n",
    "            results[model_name] = result\n",
    "    \n",
    "    # Summary comparison\n",
    "    if len(results) > 1:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"SUMMARY COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name, result in results.items():\n",
    "            if result['total_average'] is not None:\n",
    "                if thought_type == \"transfer\":\n",
    "                    display_name = f\"{result['source_model']}→{model_name}\"\n",
    "                else:\n",
    "                    display_name = f\"{model_name}({thought_type if thought_type else 'default'})\"\n",
    "                print(f\"{display_name:<35}: {result['total_average']:.4f} ({result['tasks_processed']}/{result['total_tasks']} tasks)\")\n",
    "            else:\n",
    "                print(f\"{model_name:<35}: No valid scores\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_transfer_scenarios(source_models, target_models, thought_type=\"transfer\"):\n",
    "    \"\"\"\n",
    "    Compare multiple transfer scenarios (source → target combinations).\n",
    "    \n",
    "    Args:\n",
    "        source_models (list): List of source model names\n",
    "        target_models (list): List of target model names\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for source in source_models:\n",
    "        for target in target_models:\n",
    "            if True:  # Skip self-transfer\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"TRANSFER: {source} → {target}\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                result = calculate_average_induction_scores(target, thought_type, source)\n",
    "                if result:\n",
    "                    transfer_key = f\"{source}_to_{target}\"\n",
    "                    all_results[transfer_key] = result\n",
    "    \n",
    "    # Summary of all transfers\n",
    "    if len(all_results) > 1:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TRANSFER SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Sort by average score (descending)\n",
    "        sorted_results = sorted(all_results.items(), \n",
    "                              key=lambda x: x[1]['total_average'] if x[1]['total_average'] else 0, \n",
    "                              reverse=True)\n",
    "        \n",
    "        for transfer_key, result in sorted_results:\n",
    "            if result['total_average'] is not None:\n",
    "                transfer_name = f\"{result['source_model']} → {result['model_name']}\"\n",
    "                print(f\"{transfer_name:<45}: {result['total_average']:.4f} ({result['tasks_processed']}/{result['total_tasks']} tasks)\")\n",
    "            else:\n",
    "                print(f\"{transfer_key:<45}: No valid scores\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Regular model evaluation with empty thoughts\n",
    "    model_names = [\"nvidia_Nemotron-Research-Reasoning-Qwen-1.5B\", \n",
    "                   \"open-thoughts_OpenThinker-7B\",\n",
    "                   \"openai_gpt-oss-20b\",\n",
    "                   \"Qwen_QwQ-32B\", \n",
    "                   \"BytedTsinghua-SIA_DAPO-Qwen-32B\"]\n",
    "    \n",
    "    # print(\"EMPTY THOUGHT EVALUATION:\")\n",
    "    #results_empty = compare_multiple_models(model_names, \"ensemble_without_answer_gen_qwq_opent_eval_oss\")\n",
    "    results_empty = compare_multiple_models(model_names, \"with_sampling_without_answer\")\n",
    "    \n",
    "    # Example 2: Single transfer evaluation\n",
    "    # print(f\"\\n{'='*80}\")\n",
    "    # print(\"TRANSFER EVALUATION EXAMPLE:\")\n",
    "    # source_model = \"Qwen_QwQ-32B\"\n",
    "    # target_model = \"openai_gpt-oss-20b\"\n",
    "    # transfer_result = calculate_average_induction_scores(target_model, \"transfer\", source_model)\n",
    "    \n",
    "    # Example 3: Multiple transfer scenarios\n",
    "    # print(f\"\\n{'='*80}\")\n",
    "    # print(\"MULTIPLE TRANSFER SCENARIOS:\")\n",
    "    # source_models = [\"BytedTsinghua-SIA_DAPO-Qwen-32B\"]\n",
    "    # target_models = [\"nvidia_Nemotron-Research-Reasoning-Qwen-1.5B\", \n",
    "    #                \"open-thoughts_OpenThinker-7B\",\n",
    "    #                \"openai_gpt-oss-20b\",\n",
    "    #                \"Qwen_QwQ-32B\", \n",
    "    #                \"BytedTsinghua-SIA_DAPO-Qwen-32B\"]\n",
    "    # transfer_results = compare_transfer_scenarios(source_models, target_models, \"transfer without answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d79bd1-a3dd-4f17-a7ae-5d86d35ca248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
