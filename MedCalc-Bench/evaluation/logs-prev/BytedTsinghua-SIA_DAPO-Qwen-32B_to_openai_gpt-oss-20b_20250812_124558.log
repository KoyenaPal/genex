Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/14 [00:00<00:12,  1.07it/s]Loading checkpoint shards:  14%|█▍        | 2/14 [00:02<00:13,  1.10s/it]Loading checkpoint shards:  21%|██▏       | 3/14 [00:03<00:12,  1.12s/it]Loading checkpoint shards:  29%|██▊       | 4/14 [00:04<00:11,  1.14s/it]Loading checkpoint shards:  36%|███▌      | 5/14 [00:05<00:10,  1.12s/it]Loading checkpoint shards:  43%|████▎     | 6/14 [00:06<00:08,  1.12s/it]Loading checkpoint shards:  50%|█████     | 7/14 [00:07<00:07,  1.12s/it]Loading checkpoint shards:  57%|█████▋    | 8/14 [00:08<00:06,  1.10s/it]Loading checkpoint shards:  64%|██████▍   | 9/14 [00:09<00:05,  1.09s/it]Loading checkpoint shards:  71%|███████▏  | 10/14 [00:10<00:04,  1.07s/it]Loading checkpoint shards:  79%|███████▊  | 11/14 [00:12<00:03,  1.07s/it]Loading checkpoint shards:  86%|████████▌ | 12/14 [00:13<00:02,  1.07s/it]Loading checkpoint shards:  93%|█████████▎| 13/14 [00:14<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 14/14 [00:14<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:14<00:00,  1.06s/it]
Device set to use cuda:0
MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed, we will default to dequantizing the model to bf16
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.55s/it]
Traceback (most recent call last):
  File "/MedCalc-Bench/evaluation/run_transfer_thoughts.py", line 218, in <module>
    target_llm = LLMInference(llm_name=target_model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/MedCalc-Bench/evaluation/llm_inference.py", line 69, in __init__
    self.model = transformers.pipeline(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 1008, in pipeline
    framework, model = infer_framework_load_model(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py", line 292, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 316, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5061, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5524, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
                                                         ^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 974, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 882, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(
  File "/.venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_mxfp4.py", line 221, in create_quantized_param
    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)
  File "/.venv/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 326, in dequantize
    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.venv/lib/python3.11/site-packages/transformers/integrations/mxfp4.py", line 95, in convert_moe_packed_tensors
    blocks = blocks.cuda()
             ^^^^^^^^^^^^^
KeyboardInterrupt
