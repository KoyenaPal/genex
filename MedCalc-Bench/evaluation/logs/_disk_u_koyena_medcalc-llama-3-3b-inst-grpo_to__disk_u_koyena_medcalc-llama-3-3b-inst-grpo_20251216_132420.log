OUTPUT PATH _disk_u_koyena_medcalc-llama-3-3b-inst-grpo_thoughts_to__disk_u_koyena_medcalc-llama-3-3b-inst-grpo_zero_shot_full.jsonl
Traceback (most recent call last):
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py", line 1737, in convert_slow_tokenizer
    ).converted()
      ^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py", line 1631, in converted
    tokenizer = self.tokenizer()
                ^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py", line 1624, in tokenizer
    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py", line 1600, in extract_vocab_merges_from_model
    bpe_ranks = load_tiktoken_bpe(tiktoken_url)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/tiktoken/load.py", line 158, in load_tiktoken_bpe
    contents = read_file_cached(tiktoken_bpe_file, expected_hash)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/tiktoken/load.py", line 48, in read_file_cached
    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()
                             ^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'encode'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/disk/u/koyena/MedCalc-Bench/evaluation/run_transfer_thoughts.py", line 232, in <module>
    llm = LLMInference(llm_name=model_name)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/MedCalc-Bench/evaluation/llm_inference.py", line 50, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(self.llm_name, cache_dir=self.cache_dir, legacy=False)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1144, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2070, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2316, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 154, in __init__
    super().__init__(
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 139, in __init__
    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/disk/u/koyena/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py", line 1739, in convert_slow_tokenizer
    raise ValueError(
ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
